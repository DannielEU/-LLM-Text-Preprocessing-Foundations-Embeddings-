{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65766abf",
   "metadata": {},
   "source": [
    "# Embeddings - Explicación y Experimento \n",
    "\n",
    "Este notebook reproduce las partes centrales del Capítulo 2 (Raschka)\n",
    "y añade explicaciones en primera persona, con analogías cotidianas,\n",
    "y un experimento que varía `max_length` y `stride` para mostrar el número\n",
    "de muestras generadas y por qué el solapamiento es útil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b6440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomendado: ejecutar en un entorno con torch y tiktoken instalados.\n",
    "# Si no los tienes, descomenta e instala:\n",
    "# !pip install --upgrade pip\n",
    "# !pip install torch tiktoken requests\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def ensure_package(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \n",
    ", \n",
    ", \n",
    ", pkg])\n",
    "\n",
    "print('Asegúrate de tener `torch` y `tiktoken` instalados antes de ejecutar completamente este notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bf91d",
   "metadata": {},
   "source": [
    "## Tokenización y vocabulario (explicación personal)\n",
    "\n",
    "Imaginemos que el texto es una gran biblioteca y los tokens son los libros individuales.\n",
    "La tokenización decide qué es un \n",
    ": ¿una palabra completa, un fragmento de palabra,\n",
    "o incluso una letra? Esta decisión afecta lo que el modelo \n",
    " y cómo generaliza.\n",
    "\n",
    "En términos prácticos, la tokenización determina la granularidad del idioma:\n",
    "- Un token demasiado grande (p. ej. frases completas) hará que sea difícil aprender\n",
    "  relaciones porque hay demasiadas unidades distintas.\n",
    "- Un token demasiado pequeño (p. ej. letras) requiere secuencias muy largas.\n",
    "\n",
    "Por eso los tokenizers tipo BPE (usado por `tiktoken`) mezclan lo mejor: usan subpalabras\n",
    "para manejar palabras nuevas sin inflar el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f57e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar con tiktoken (BPE / GPT-2)\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "sample = raw_text[:1000]\n",
    "ids = tokenizer.encode(sample)\n",
    "print('Tokens sample length:', len(ids))\n",
    "print('Primeros 20 token ids:', ids[:20])\n",
    "print('Decoded (reconstruido):', tokenizer.decode(ids[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e877c91",
   "metadata": {},
   "source": [
    "## De tokens a IDs y embeddings (explicación personal con analogía)\n",
    "\n",
    "Ahora imaginemos que cada token tiene una ficha con un número (ID). Esa ficha es lo que\n",
    "llevamos a la máquina. Pero las máquinas no entienden fichas numeradas: necesitan vectores,\n",
    "es decir, una lista de características numéricas. La capa de `Embedding` es como una mesa\n",
    "donde cada ficha (ID) te da una tarjeta con información (vector).\n",
    "\n",
    "así: si los tokens son personas, la embedding es su tarjeta de presentación\n",
    "con rasgos (edad, oficio, intereses) que facilitan encontrar afinidad entre ellas.\n",
    "Cuanto más parecidas sean las tarjetas, más cerca estarán en el espacio vectorial.\n",
    "\n",
    "Así lo entiendo yo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d41d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset tipo sliding-window (adaptado del libro)\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        # tokenizar todo el texto con tiktoken (ya es BPE)\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\n",
    "})\n",
    "        assert len(token_ids) > max_length, 'tokenized length must be > max_length'\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7398b0a",
   "metadata": {},
   "source": [
    "## Sliding window y solapamiento (explicación con ejemplo diario)\n",
    "\n",
    "Imagínemos que queremos aprender una canción larga memorizando fragmentos. Si memorizamos\n",
    "segmentos que se solapan (repitimos la última frase del fragmento anterior), te será más fácil\n",
    "reconstruir la canción completa porque recuerdas la unión entre fragmentos.\n",
    "\n",
    "En ML, ese solapamiento (stride pequeño) genera más ejemplos y ayuda al modelo a ver cómo\n",
    "las fronteras entre fragmentos encajan. Pero hay un trade-off: demasiada repetición =\n",
    "menos información nueva por ejemplo y riesgo de sobreajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento: variar max_length y stride y reportar número de samples\n",
    "params = [(128,128),(128,64),(128,32),(256,256),(256,128),(256,64)]\n",
    "results = []\n",
    "for max_length,stride in params:\n",
    "    ds = GPTDatasetV1(raw_text, tokenizer, max_length=max_length, stride=stride)\n",
    "    results.append((max_length,stride,len(ds)))\n",
    "\n",
    "print('max_length, stride -> num_samples')\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n",
    "# Observación: mostramos también la cantidad total de tokens y la fórmula esperada aproximada:\n",
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "print('\n",
    "Total tokens (approx):', total_tokens)\n",
    "\n",
    "# Una explicación rápida automatizada por cada experimento:\n",
    "for max_length,stride,n in results:\n",
    "    possible_positions = max(0, total_tokens - max_length)\n",
    "    expected = (possible_positions // stride)\n",
    "    print(f'For max_length={max_length:3}, stride={stride:3} -> samples={n:6} (approx expected={expected})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990733b",
   "metadata": {},
   "source": [
    "## Resultado del experimento (explicación personal)\n",
    "\n",
    "- `max_length` define cuánto contexto ve el modelo en cada ejemplo (como cuánto de la canción recuerdas).\n",
    "- `stride` controla cuánto avanzamos entre ejemplos; `stride < max_length` implica solapamiento.\n",
    "\n",
    "Analogía: si `max_length` es el tamaño de tu hoja de estudio y `stride` cuánto avanzamos al pasar la hoja,\n",
    "un `stride` pequeño significa que volvemos a leer partes que ya vimos (refuerzo), mientras que un `stride` grande\n",
    "significa leer contenido mayormente nuevo (mayor diversidad)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b673794",
   "metadata": {},
   "source": [
    "## ¿Por qué las embeddings codifican significado? \n",
    "\n",
    "Voy a explicarlo como si yo se lo contara a un compañero:\n",
    "\n",
    "1) Hipótesis distribucional (intuitiva):\n",
    "- He notado que palabras que aparecen en contextos similares (por ejemplo, \"perro\" y \"gato\") tienden\n",
    "  a estar relacionadas; las embeddings capturan esa idea transformando co-ocurrencias en distancias.\n",
    "\n",
    "2) Analogía de la vida diaria:\n",
    "- Piensa en una fiesta donde cada persona recibe una tarjeta con varios rasgos: \"le gusta el jazz\", \"es vegetariano\",\n",
    "  \"trabaja en software\". Las personas con tarjetas parecidas tienden a agruparse. Una embedding es exactamente esa tarjeta:\n",
    "  un vector de rasgos que facilita encontrar afinidades.\n",
    "\n",
    "3) Relación con redes neuronales:\n",
    "- Una embedding es simplemente una fila de la matriz de pesos W en la capa `Embedding`.\n",
    "  Cuando entrenas la red por backprop, estas filas (vectores) cambian para que las proximidades en el espacio\n",
    "  reflejen relaciones útiles para la tarea.\n",
    "\n",
    "4) Contextualización:\n",
    "- Una embedding estática (por token) captura información general; sin embargo, los modelos modernos generan\n",
    "  embeddings contextuales (la misma palabra tiene vectores distintos según la frase). Estos últimos codifican significado\n",
    "  dinámico y son más precisos para tareas complejas.\n",
    "\n",
    "5) Limitaciones y sentido práctico: \n",
    "- Embeddings no son magia: capturan correlaciones del dato de entrenamiento. Si el corpus está sesgado, las embeddings\n",
    "  también lo estarán. Además, la dimensión (p. ej. 256 vs 1024) es un trade-off entre capacidad y coste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar creación de embeddings token -> vector y su forma\n",
    "import torch\n",
    "vocab_size = 50257  # GPT-2 BPE vocab size (tiktoken)\n",
    "output_dim = 256\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "# Tomemos algunos token ids de ejemplo y los embedemos\n",
    "example_ids = torch.tensor(ids[:8], dtype=torch.long)\n",
    "embs = embedding_layer(example_ids)\n",
    "print('Example ids:', example_ids.tolist())\n",
    "print('Embeddings shape:', embs.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
